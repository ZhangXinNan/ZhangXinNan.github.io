<!DOCTYPE html>
<html lang="ch">
<head>
        <meta charset="utf-8" />
        <title>WilledIt</title>
        <link rel="stylesheet" href="http://zhangxinnan.github.io/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="http://zhangxinnan.github.io/">WilledIt </a></h1>
                <nav><ul>
                    <li><a href="http://zhangxinnan.github.io/category/deeplearning.html">DeepLearning</a></li>
                    <li><a href="http://zhangxinnan.github.io/category/machinelearning.html">MachineLearning</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://zhangxinnan.github.io/LinearRegressionWithOneVariable.html">单变量线性回归</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-06-24T00:00:00+08:00">
                Published: 五 24 六月 2016
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://zhangxinnan.github.io/author/zhangxinnan.html">ZhangXinNan</a>
        </address>
<p>In <a href="http://zhangxinnan.github.io/category/machinelearning.html">MachineLearning</a>.</p>
<p>tags: <a href="http://zhangxinnan.github.io/tag/andrewng.html">AndrewNg</a> </p>
</footer><!-- /.post-info --><h1>二、单变量线性回归（Linear Regression with One Variable）</h1>
<h2>2.1 模型表示</h2>
<p><strong>notation</strong>(符号):
<em> m = Number of trainging examples
</em> x's = "input" variable / features (输入变量或者特征)
<em> y's = "output" variable / "target" variable (输出变量或者目标变量)
</em> (x,y) = one training example (一个训练样本)
<em> (<span class="math">\(x^i\)</span>, <span class="math">\(y^i\)</span>) = <span class="math">\(i^{th}\)</span> training example (第i个训练样本)
</em> h = hypothesis (假设)</p>
<p>单变量性线回归：
</p>
<div class="math">$$h_\theta(x) = \theta_0 + \theta_1 x$$</div>
<div class="highlight"><pre><span></span>st=&gt;start: Training Set
op=&gt;operation: Learning Algorithm
op1=&gt;end: hypothesis(假设)
x=&gt;inputoutput: sizeof house
y=&gt;inputoutput: estimated price

st-&gt;op-&gt;op1
</pre></div>


<div class="highlight"><pre><span></span>input-&gt;hypothesis:sizeof house
hypothesis--&gt;output:estimated price
</pre></div>


<h2>2.2 cost function</h2>
<p>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确度，模型所预测的值与训练集中实际值之间的差距就是<strong><em>建模误差</em></strong>（modeling error）。</p>
<h2>2.3 cost function - intuition 1</h2>
<p>Hypothesis(假设函数):
  </p>
<div class="math">$$h_{\theta}(x^i)=\theta_0 + \theta_1x$$</div>
<p>
Parameters :
  </p>
<div class="math">$$\theta_0, \theta_1$$</div>
<p>
Cost Function(代价函数) :
  </p>
<div class="math">$$J(\theta_0, \theta_1)=\frac1{2m}\sum_{i=1}^m(h_{\theta}(x^i) - y ^i)^2$$</div>
<p>
Goal :
  </p>
<div class="math">$$minimize_{\theta_0, \theta_1}J(\theta_0, \theta_1)$$</div>
<h2>2.4 cost function - intuition 2</h2>
<h2>2.5 Gradient Descent</h2>
<p>梯度下降是一个用来求函数最小值的算法。</p>
<p>梯度下降背后的思想是:开始时我们随机选择一个参数的组合(θ0,θ1,...,θn),计算代价 函数,然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到 一个<strong><em>局部最小值</em></strong>(local minimum),因为我们并没有尝试完所有的参数组合,所以不能确定 我们得到的局部最小值是否便是<strong><em>全局最小值</em></strong>(global minimum),选择不同的初始参数组合, 可能会找到不同的局部最小值。</p>
<hr />
<p>批量梯度下降（batch gradient descent）算法的公式：</p>
<p>repeat until convergence {
  </p>
<div class="math">$$\theta_j := \theta_j - \alpha \frac\partial{\partial\theta_j}J(\theta_0, \theta_1)$$</div>
<p>
  for j = 0 and j = 1
}
<span class="math">\(\alpha\)</span> 是学习率 learning rate，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。</p>
<hr />
<p><strong>Gradient descent algorithm梯度下降算法</strong> :
</p>
<div class="math">$$\theta_j := \theta_j - \alpha \frac{\partial J(\theta_0,\theta_1)}{\partial{\theta_{j}}} $$</div>
<p>
说明：
<span class="math">\(\alpha\)</span> ： learning rate
<span class="math">\(:=\)</span> : assignment
Correct : Simultaneous update (<strong>同步更新是正确的</strong>)
</p>
<div class="math">$$temp0 := \theta_0 - \alpha \frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1)$$</div>
<div class="math">$$temp1 := \theta_1 - \alpha \frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)$$</div>
<div class="math">$$\theta_0 := temp0$$</div>
<div class="math">$$\theta_1 := temp1$$</div>
<h2>2.6 gradient Descent intuition</h2>
<h2>2.7 gradient Descent for linear regression</h2>
<p><strong>Gradient descent algorithm :</strong></p>
<p>repeat until convergence {
   </p>
<div class="math">$$\theta_j := \theta_j - \alpha \frac\partial{\partial\theta_j}J(\theta_0, \theta_1)$$</div>
<p>
   for j = 1 and j = 0
 }</p>
<hr />
<p><strong>Linear Regression Model :</strong>
  </p>
<div class="math">$$h_{\theta}(x) = \theta_0 + \theta_1 x$$</div>
<div class="math">$$J(\theta_0, \theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^i) - y^i)^2$$</div>
<p>
对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：
</p>
<div class="math">$$J(\theta_0, \theta_1)=\frac{1}{2m}\sum^m_{i=1}(\theta_0 + \theta_1x^i - y^i)^2$$</div>
<p>
<span class="math">\(j = 0 :\frac{\partial}{\partial {\theta_0}}J(\theta_0, \theta_1) =  \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^i) - y^i)\)</span>
<span class="math">\(j = 1 : \frac{\partial}{\partial {\theta_1}}J(\theta_0, \theta_1) =  \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^i) - y^i)x^i\)</span></p>
<p>则算法改写成：
Repeat {
  </p>
<div class="math">$$\theta_0 := \theta_0 - \alpha \frac 1 m \sum^m_{i=1}(h_\theta(x^i) - y^i)$$</div>
<div class="math">$$\theta_1 := \theta_1 - \alpha \frac 1 m \sum^m_{i=1}(h_\theta(x^i) - y^i)x^i$$</div>
<p>
}</p>
<p>Batch Graddient Descent 批量梯度下降 :
"Batch" : each step of gradient descent uses all the training examples.</p>
<h2>2.8 接下来的内容</h2>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="http://zhangxinnan.github.io/ArtificialNeuralNetwork.html" rel="bookmark"
                           title="Permalink to 人工神经网络">人工神经网络</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2016-06-23T00:00:00+08:00">
                Published: 四 23 六月 2016
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://zhangxinnan.github.io/author/zhangxinnan.html">ZhangXinNan</a>
        </address>
<p>In <a href="http://zhangxinnan.github.io/category/deeplearning.html">DeepLearning</a>.</p>
<p>tags: <a href="http://zhangxinnan.github.io/tag/neuralnetwork.html">NeuralNetwork</a> </p>
</footer><!-- /.post-info -->                <h3>1 人工神经网络简介</h3>
<h4>生物学动机</h4>
<p>人工神经网络ANN的研究一定程度上受到了生物学的启发，生物的学习系统由相互连接的神经元（neuron）组成的异常复杂的网格。而人工神经网络由一系列简单的单元相互密集连接构成的，其中每一个单元有一定数量的实值输入，并产生单一的实数值输出。据估计人类的大脑是由大约<span class="math">\(10^{11}\)</span>次方个神经元相互连接组成的密集网络，平均每个神经元与其他<span class="math">\(10^4\)</span>个神经元相连。神经元的活性通常被通向其他神经元的连接激活或抑制。</p>
<h3>2 神经网络表示</h3>
<p>1993年的ALVINN系统是ANN学习的一个典型实例，这个系统使用一个学习到的ANN以正常的速度在高速公路上驾驶汽车。ANN的输入是一个30*32像素的网格，像素的亮度来自一个安装在车辆上的前向摄像机。ANN的输出是车辆行驶的方向。
<img alt="ALVINN" src="images/ALVINN.jpg" /></p>
<h3>3 适合神经网络学习的问题</h3>
<ul>
<li>实例是用很多属性-值表示的</li>
<li>目标函数的输出可能是离散值、实数值或者由若干实数属性或者离散属性组成的向量</li>
<li>训练数据可能包含错误</li>
<li>可容忍长时间的训练</li>
<li>可能需要快速求出目标函数值</li>
<li>人类能否理解学到的目标函数不是重要的。</li>
</ul>
<h3>4 感知器</h3>
<p>感知器以一个实数值向量作为输入，计算这些输入的线性组合 ，然后如果结果大于某个阈值，就输出1，否则输出-1。
</p>
<div class="math">$$o(x_1, x_2,...,x_n) = \begin{equation}
\begin ...</div>
                <a class="readmore" href="http://zhangxinnan.github.io/ArtificialNeuralNetwork.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://weibo.com/sdlypyzq">cv_ml_张欣男</a></li>
                            <li><a href="https://www.zhihu.com/people/zhangxinnan">张欣</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>