<!DOCTYPE html>
<html lang="ch">
<head>
        <meta charset="utf-8" />
        <title>WilledIt - 机器学习</title>
        <link rel="stylesheet" href="http://zhangxinnan.github.io/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="http://zhangxinnan.github.io/">WilledIt </a></h1>
                <nav><ul>
                    <li><a href="http://zhangxinnan.github.io/category/misc.html">misc</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://zhangxinnan.github.io/ji-qi-xue-xi-dan-bian-liang-xian-xing-hui-gui.html">机器学习——单变量线性回归</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-06-23T00:00:00+08:00">
                Published: 四 23 六月 2016
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://zhangxinnan.github.io/author/zhangxinnan.html">ZhangXinNan</a>
        </address>
<p>In <a href="http://zhangxinnan.github.io/category/misc.html">misc</a>.</p>
<p>tags: <a href="http://zhangxinnan.github.io/tag/ji-qi-xue-xi.html">机器学习</a> </p>
</footer><!-- /.post-info --><h1>二、单变量线性回归（Linear Regression with One Variable）</h1>
<h2>2.1 模型表示</h2>
<p><strong>notation</strong>(符号):
<em> m = Number of trainging examples
</em> x's = "input" variable / features (输入变量或者特征)
<em> y's = "output" variable / "target" variable (输出变量或者目标变量)
</em> (x,y) = one training example (一个训练样本)
<em> ($x^i$, $y^i$) = $i^{th}$ training example (第i个训练样本)
</em> h = hypothesis (假设)</p>
<p>单变量性线回归：
$$h_\theta(x) = \theta_0 + \theta_1 x$$</p>
<div class="highlight"><pre><span></span>st=&gt;start: Training Set
op=&gt;operation: Learning Algorithm
op1=&gt;end: hypothesis(假设)
x=&gt;inputoutput: sizeof house
y=&gt;inputoutput: estimated price

st-&gt;op-&gt;op1
</pre></div>


<div class="highlight"><pre><span></span>input-&gt;hypothesis:sizeof house
hypothesis--&gt;output:estimated price
</pre></div>


<h2>2.2 cost function</h2>
<p>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确度，模型所预测的值与训练集中实际值之间的差距就是<strong><em>建模误差</em></strong>（modeling error）。</p>
<h2>2.3 cost function - intuition 1</h2>
<p>Hypothesis(假设函数):
  $$h_{\theta}(x^i)=\theta_0 + \theta_1x$$
Parameters :
  $$\theta_0, \theta_1$$
Cost Function(代价函数) :
  $$J(\theta_0, \theta_1)=\frac1{2m}\sum_{i=1}^m(h_{\theta}(x^i) - y ^i)^2$$
Goal :
  $$minimize_{\theta_0, \theta_1}J(\theta_0, \theta_1)$$</p>
<h2>2.4 cost function - intuition 2</h2>
<h2>2.5 Gradient Descent</h2>
<p>梯度下降是一个用来求函数最小值的算法。</p>
<p>梯度下降背后的思想是:开始时我们随机选择一个参数的组合(θ0,θ1,...,θn),计算代价 函数,然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到 一个<strong><em>局部最小值</em></strong>(local minimum),因为我们并没有尝试完所有的参数组合,所以不能确定 我们得到的局部最小值是否便是<strong><em>全局最小值</em></strong>(global minimum),选择不同的初始参数组合, 可能会找到不同的局部最小值。</p>
<hr />
<p>批量梯度下降（batch gradient descent）算法的公式：</p>
<p>repeat until convergence {
  $$\theta_j := \theta_j - \alpha \frac\partial{\partial\theta_j}J(\theta_0, \theta_1)$$
  for j = 0 and j = 1
}
$\alpha$ 是学习率 learning rate，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。</p>
<hr />
<p><strong>Gradient descent algorithm梯度下降算法</strong> :
$$\theta_j := \theta_j - \alpha \frac{\partial J(\theta_0,\theta_1)}{\partial{\theta_{j}}} $$
说明：
$\alpha$ ： learning rate
$:=$ : assignment
Correct : Simultaneous update (<strong>同步更新是正确的</strong>)
$$temp0 := \theta_0 - \alpha \frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1)$$
$$temp1 := \theta_1 - \alpha \frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)$$
$$\theta_0 := temp0$$
$$\theta_1 := temp1$$</p>
<h2>2.6 gradient Descent intuition</h2>
<h2>2.7 gradient Descent for linear regression</h2>
<p><strong>Gradient descent algorithm :</strong></p>
<p>repeat until convergence {
   $$\theta_j := \theta_j - \alpha \frac\partial{\partial\theta_j}J(\theta_0, \theta_1)$$
   for j = 1 and j = 0
 }</p>
<hr />
<p><strong>Linear Regression Model :</strong>
  $$h_{\theta}(x) = \theta_0 + \theta_1 x$$
  $$J(\theta_0, \theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^i) - y^i)^2$$
对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：
$$J(\theta_0, \theta_1)=\frac{1}{2m}\sum^m_{i=1}(\theta_0 + \theta_1x^i - y^i)^2$$
$j = 0 :\frac{\partial}{\partial {\theta_0}}J(\theta_0, \theta_1) =  \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^i) - y^i)$
$j = 1 : \frac{\partial}{\partial {\theta_1}}J(\theta_0, \theta_1) =  \frac{1}{m}\sum^m_{i=1}(h_{\theta}(x^i) - y^i)x^i$</p>
<p>则算法改写成：
Repeat {
  $$\theta_0 := \theta_0 - \alpha \frac 1 m \sum^m_{i=1}(h_\theta(x^i) - y^i)$$
  $$\theta_1 := \theta_1 - \alpha \frac 1 m \sum^m_{i=1}(h_\theta(x^i) - y^i)x^i$$
}</p>
<p>Batch Graddient Descent 批量梯度下降 :
"Batch" : each step of gradient descent uses all the training examples.</p>
<h2>2.8 接下来的内容</h2>
<p><a href="./week01_03.html">线性代数回顾</a></p>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="http://zhangxinnan.github.io/ji-qi-xue-xi-xian-xing-dai-shu-hui-gu.html" rel="bookmark"
                           title="Permalink to 机器学习——线性代数回顾">机器学习——线性代数回顾</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2016-06-23T00:00:00+08:00">
                Published: 四 23 六月 2016
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://zhangxinnan.github.io/author/zhangxinnan.html">ZhangXinNan</a>
        </address>
<p>In <a href="http://zhangxinnan.github.io/category/misc.html">misc</a>.</p>
<p>tags: <a href="http://zhangxinnan.github.io/tag/ji-qi-xue-xi.html">机器学习</a> </p>
</footer><!-- /.post-info -->                <h1>三、线性代数回顾</h1>
<h2>3.1 矩阵和向量</h2>
                <a class="readmore" href="http://zhangxinnan.github.io/ji-qi-xue-xi-xian-xing-dai-shu-hui-gu.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>